{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw_11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "# 3. Бибилотека чтобы разделять грамотно иероглифы\n",
        "import jieba"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNvjhDyAKk3U",
        "outputId": "8f14501d-08bd-4c34-f69a-2aad31006baf"
      },
      "source": [
        "!wget http://www.manythings.org/anki/cmn-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-10 10:31:18--  http://www.manythings.org/anki/cmn-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3033::ac43:ba36, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1153006 (1.1M) [application/zip]\n",
            "Saving to: ‘cmn-eng.zip’\n",
            "\n",
            "cmn-eng.zip         100%[===================>]   1.10M  3.88MB/s    in 0.3s    \n",
            "\n",
            "2021-10-10 10:31:19 (3.88 MB/s) - ‘cmn-eng.zip’ saved [1153006/1153006]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bg17Lr-7XK",
        "outputId": "7490942c-ede9-4739-9147-89a8fdd807d8"
      },
      "source": [
        "!mkdir cn-eng\n",
        "!unzip cmn-eng.zip -d cn-eng/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  cmn-eng.zip\n",
            "  inflating: cn-eng/cmn.txt          \n",
            "  inflating: cn-eng/_about.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o5L92efMMhf",
        "outputId": "724f6a25-ec7e-4870-b62e-f548ab55d1f6"
      },
      "source": [
        "\n",
        "!ls /content/cn-eng/ -lah"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.7M\n",
            "drwxr-xr-x 2 root root 4.0K Oct 10 10:31 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct 10 10:31 ..\n",
            "-rw-r--r-- 1 root root 1.5K Jul 14 10:16 _about.txt\n",
            "-rw-r--r-- 1 root root 3.6M Jul 14 10:16 cmn.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "source": [
        "# Download the file\n",
        "path_to_file = \"/content/cn-eng/cmn.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "source": [
        "# 2. Потом я заметил что модель выплевывает пустые строки, думаю что надо разделять иероглифы каждый между собой. Добавим процедуру cutword и unicode_to_ascii в функцию preprocess_sentence\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "    \n",
        "def cutword(sentence):\n",
        "    output = []\n",
        "    for word in jieba.cut(sentence, cut_all=False):\n",
        "        output.append(word)\n",
        "    output = ' '.join(output)\n",
        "    return output\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.。!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,？。！，\\u4e00-\\u9FFF]+\", \" \", w)\n",
        "  if len(re.findall('([a-z])',w)) == 0:\n",
        "        try:\n",
        "            w = cutword(w)\n",
        "        except:\n",
        "            pass\n",
        "  \n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfGQ7c-_cKT4"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# cut chinese\n",
        "def cutword(sentence):\n",
        "    output = []\n",
        "    for word in jieba.cut(sentence, cut_all=False):\n",
        "        output.append(word)\n",
        "    output = ' '.join(output)\n",
        "    return output\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    w = re.sub(r\"([?.!,？。！，])\", r\" \\1 \", w)\n",
        "    # delete extra spaces\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    # as well as Chinese characters\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,？。！，\\u4e00-\\u9FFF]+\", \" \", w)\n",
        "\n",
        "    # cut words\n",
        "    if len(re.findall('([a-z])',w)) == 0:\n",
        "        try:\n",
        "            w = cutword(w)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "yV9lZXQXNbnH",
        "outputId": "2b5f89b2-2178-42eb-d4f6-a5f5b1e8f3bf"
      },
      "source": [
        "cutword(\"我是你大爷。\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.830 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'我 是 你 大爷 。'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm0OXZ5hcQyA",
        "outputId": "f9232663-3ee7-4498-b2c8-1bf0634bc6c8"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "zh_sentence = u\"我可以借这本书吗？\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(zh_sentence))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> may i borrow this book ?  <end>\n",
            "<start> 我 可以 借 这 本书 吗   ？   <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ3OrQ6anwIZ"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = []\n",
        "    for line in lines[:num_examples]:\n",
        "        sentences = line.split('\\t')[:2]\n",
        "        word_pairs.append((preprocess_sentence(sentences[0]),\n",
        "                           preprocess_sentence(sentences[1])))\n",
        "    return word_pairs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIuJzXd7oEyq",
        "outputId": "c9d3ef14-193e-418f-c189-90fcf2e72762"
      },
      "source": [
        "for l,k in create_dataset(path_to_file, 10):\n",
        "    print(l)\n",
        "    print(k)\n",
        "    print('English length: {}; Chinese length: {}'.format(len(l),len(k)))\n",
        "    print('--' *30)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> hi .  <end>\n",
            "<start> 嗨   。   <end>\n",
            "English length: 19; Chinese length: 21\n",
            "------------------------------------------------------------\n",
            "<start> hi .  <end>\n",
            "<start> 你好   。   <end>\n",
            "English length: 19; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> run .  <end>\n",
            "<start> 你 用 跑 的   。   <end>\n",
            "English length: 20; Chinese length: 27\n",
            "------------------------------------------------------------\n",
            "<start> wait !  <end>\n",
            "<start> 等等   ！   <end>\n",
            "English length: 21; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> wait !  <end>\n",
            "<start> 等 一下   ！   <end>\n",
            "English length: 21; Chinese length: 24\n",
            "------------------------------------------------------------\n",
            "<start> begin .  <end>\n",
            "<start> 开始   ！   <end>\n",
            "English length: 22; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> hello !  <end>\n",
            "<start> 你好   。   <end>\n",
            "English length: 22; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> i try .  <end>\n",
            "<start> 我 试试   。   <end>\n",
            "English length: 22; Chinese length: 24\n",
            "------------------------------------------------------------\n",
            "<start> i won !  <end>\n",
            "<start> 我 赢 了   。   <end>\n",
            "English length: 22; Chinese length: 25\n",
            "------------------------------------------------------------\n",
            "<start> oh no !  <end>\n",
            "<start> 不会 吧   。   <end>\n",
            "English length: 22; Chinese length: 24\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFR6khs6o1vK",
        "outputId": "0ed18acf-0c21-402f-b2d2-140861b27338"
      },
      "source": [
        "english = []\n",
        "chinese = []\n",
        "\n",
        "for en, zh in create_dataset(path_to_file, None):\n",
        "    english.append(en)\n",
        "    chinese.append(zh)\n",
        "\n",
        "print('--------- Original ---------')\n",
        "with open (path_to_file) as f:\n",
        "    for i in (f.read().split('\\n')[-2].split('\\t')):\n",
        "        print(i)\n",
        "print('')\n",
        "print('--------- Processed ---------')\n",
        "print(english[-1])\n",
        "print(chinese[-1])\n",
        "print('')\n",
        "print('Number of English Sentences: ', len(english))\n",
        "print('Number of Chinese sentences: ', len(chinese))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- Original ---------\n",
            "If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\n",
            "如果一個人在成人前沒有機會習得目標語言，他對該語言的認識達到母語者程度的機會是相當小的。\n",
            "CC-BY 2.0 (France) Attribution: tatoeba.org #1230633 (alec) & #1205914 (cienias)\n",
            "\n",
            "--------- Processed ---------\n",
            "<start> if a person has not had a chance to acquire his target language by the time he s an adult , he s unlikely to be able to reach native speaker level in that language .  <end>\n",
            "<start> 如果 一個 人 在 成人 前 沒 有 機會習 得 目標 語言   ，   他 對 該 語言 的 認識 達 到 母語者 程度 的 機會 是 相當 小 的   。   <end>\n",
            "\n",
            "Number of English Sentences:  26388\n",
            "Number of Chinese sentences:  26388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4vgafsXpFFF"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "source": [
        "# create a helper function to get the padded tensor length\n",
        "# the default level(0.98) means that 98% of all sentences have fewer than n tokens\n",
        "def get_pad_len(tensor, level=0.98): \n",
        "    n = 0\n",
        "    while True:\n",
        "        count = 0\n",
        "        for i in tensor:\n",
        "            if len(i) < n:\n",
        "                count += 1\n",
        "        if count / len(tensor) >= level:\n",
        "            break\n",
        "        n += 1\n",
        "    return n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ukpjRBbpsy1"
      },
      "source": [
        "def to_tensor(lang, return_tensor=True, return_tokenizer=False):\n",
        "    # Assigns the index (sequence) of each word in a text to X\n",
        "    tokenizer = Tokenizer(filters=' ', oov_token='<OOV>') \n",
        "    tokenizer.fit_on_texts(lang)\n",
        "    lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "    lang_tensor = pad_sequences(lang_tensor,\n",
        "                                maxlen=get_pad_len(lang_tensor), # use the previously created function\n",
        "                                padding='post',\n",
        "                                truncating='post') \n",
        "    if return_tensor:\n",
        "        print('\\nShape of data tensor:', lang_tensor.shape)\n",
        "        return lang_tensor\n",
        "    if return_tokenizer:\n",
        "        return tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB3f-h8EpwqP",
        "outputId": "ce215416-4029-44ac-c505-f6a03063be07"
      },
      "source": [
        "english_tokenizer = to_tensor(english, False, True)\n",
        "english_tensor = to_tensor(english)\n",
        "print('\\nOriginal sentence:')\n",
        "print(english[500])\n",
        "print('\\nTensor of the sentence:')\n",
        "print(english_tensor[500])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of data tensor: (26388, 17)\n",
            "\n",
            "Original sentence:\n",
            "<start> tom laughed .  <end>\n",
            "\n",
            "Tensor of the sentence:\n",
            "[  2  12 944   4   3   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj_d4yQ3qfX3",
        "outputId": "f80e9d8b-8abb-45da-8c08-1b3919c6bb05"
      },
      "source": [
        "\n",
        "chinese_tokenizer = to_tensor(chinese, False, True)\n",
        "chinese_tensor = to_tensor(chinese)\n",
        "print('\\nOriginal sentence:')\n",
        "print(chinese[500])\n",
        "print('\\nTensor of the sentence:')\n",
        "print(chinese_tensor[500])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of data tensor: (26388, 16)\n",
            "\n",
            "Original sentence:\n",
            "<start> 汤姆 笑 了   。   <end>\n",
            "\n",
            "Tensor of the sentence:\n",
            "[  2  14 324   7   4   3   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMQmnjie4mFH",
        "outputId": "e4b72472-a175-48a4-f6b4-fad2d529a9e7"
      },
      "source": [
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "chinese_vocab_size = len(chinese_tokenizer.word_index) + 1\n",
        "\n",
        "print('Found {} unique tokens in English.\\n'.format(english_vocab_size))\n",
        "print('Found {} unique tokens in Chinese.\\n'.format(chinese_vocab_size))\n",
        "\n",
        "name_dict = ['Chinese', 'English']\n",
        "for idx, lang in enumerate([chinese_tokenizer, english_tokenizer]):\n",
        "    print('The 10 most frequent tokens in {} are:'.format(name_dict[idx]))\n",
        "    for idx, word in enumerate(lang.word_index):\n",
        "        if word not in ['<OOV>' , '<start>', '<end>']:\n",
        "            print(word, end='|')\n",
        "        if idx == 13:\n",
        "            break\n",
        "    print('\\n')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6756 unique tokens in English.\n",
            "\n",
            "Found 15400 unique tokens in Chinese.\n",
            "\n",
            "The 10 most frequent tokens in Chinese are:\n",
            "。|我|的|了|你|他|？|在|是|她|汤姆|\n",
            "\n",
            "The 10 most frequent tokens in English are:\n",
            ".|i|the|to|you|a|?|is|tom|t|he|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbUR8g9R4wDh"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbY0N5Ht4vJ5"
      },
      "source": [
        "english_train, english_test, chinese_train, chinese_test = train_test_split(\n",
        "    english_tensor, chinese_tensor, test_size=0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "BUFFER_SIZE = len(english_train)\n",
        "BATCH_SIZE = 64\n",
        "STEPS_PER_EPOCH = len(english_train)//BATCH_SIZE\n",
        "EMBEDDING_DIM = 128\n",
        "ENC_HIDDEN_DIM = 1024\n",
        "DEC_HIDDEN_DIM = 1024\n",
        "\n",
        "# creating a TensorFlow Dataset object \n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_train, chinese_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# batching\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9fcf63-c0af-4965-8e46-063464558c3d"
      },
      "source": [
        "print(chinese_vocab_size)\n",
        "print(english_vocab_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15400\n",
            "6756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYs4fsBh5Ucv"
      },
      "source": [
        "# Encoder-decoder + Attantion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_hidden_dim, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_hidden_dim = enc_hidden_dim\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    # 1. Добавил  tf.keras.layers.Dropout в encoder для нормализации.\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_hidden_dim,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "  def call(self, x, init_state, training=True):\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    output, hidden = self.gru(x, initial_state = init_state)\n",
        "    return output, hidden\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_hidden_dim))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0kVsb9wvzm"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    \"\"\" bahdanau-style assistive attention \"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(units)\n",
        "        self.d2 = tf.keras.layers.Dense(units)\n",
        "        self.d3 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_3d = tf.expand_dims(query, 1)\n",
        "\n",
        "        scores = self.d3(tf.nn.tanh(\n",
        "            self.d1(query_3d) + self.d2(values)))\n",
        "        # values.shape == (64, 17, 1024)\n",
        "\n",
        "        # tfa.seq2seq.LuongAttention\n",
        "        # tfa.seq2seq.BahdanauAttention\n",
        "\n",
        "        # axis 1 is the time axis, i.e. across multiple time steps\n",
        "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "\n",
        "    self.attention =  Attention(self.dec_units)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "  def call(self, x, query, value, drop=True):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    context_vector, attention_weights = self.attention(query, value)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, training=drop)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQaITYQ455vW"
      },
      "source": [
        "# Instantiate the models with sample batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "source": [
        "\n",
        "encoder = Encoder(english_vocab_size, EMBEDDING_DIM, ENC_HIDDEN_DIM, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKcypC0AGeLR",
        "outputId": "1ae29784-75f5-41fd-f95a-0d0de325ebbe"
      },
      "source": [
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 17, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y0HF-zMF_vp",
        "outputId": "503ca3ba-4d75-4687-bb6b-c29f219f827e"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  864768    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  multiple                  3545088   \n",
            "=================================================================\n",
            "Total params: 4,409,856\n",
            "Trainable params: 4,409,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjqb9nmzxcEz",
        "outputId": "983b672b-43eb-4802-ac3d-0f95341edaad"
      },
      "source": [
        "attention_layer = Attention(1024)\n",
        "context_vector, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Context vector shape: (batch size, units) {}\".format(context_vector.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 17, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oj1KwaQ8Rp2",
        "outputId": "5c0ceb02-488e-406f-b36e-30543fa4f045"
      },
      "source": [
        "decoder = Decoder(chinese_vocab_size, EMBEDDING_DIM, DEC_HIDDEN_DIM)\n",
        "\n",
        "sample_decoder_output, _, l_  = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 15400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUH4EtcoxgW7",
        "outputId": "47f1a27a-4795-4c72-bdbf-f440e36c9a0f"
      },
      "source": [
        "attention_layer.count_params()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2100225"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS_eIbeG_E06",
        "outputId": "7670fbe3-03cd-4222-9534-5f1cd74a4475"
      },
      "source": [
        "decoder.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "attention_5 (Attention)      multiple                  2100225   \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      multiple                  1971200   \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  multiple                  6690816   \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             multiple                  15785000  \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          multiple                  0         \n",
            "=================================================================\n",
            "Total params: 26,547,241\n",
            "Trainable params: 26,547,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_nmt_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMjsyyWX_T6L"
      },
      "source": [
        "Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([chinese_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    random_number = np.random.rand()\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, att_weights = decoder(dec_input, dec_hidden, enc_output)\n",
        "      \n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddefjBMa3jF0",
        "outputId": "fd79dd00-3312-45ac-eac4-9b0f049e2a5b"
      },
      "source": [
        "steps_per_epoch = len(english_train)//BATCH_SIZE\n",
        "\n",
        "with tf.device(\"/gpu:0\"):\n",
        "    EPOCHS = 50\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "            batch_loss = train_step(inp, targ, enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                        batch,\n",
        "                                                        batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.6705\n",
            "Epoch 1 Batch 100 Loss 2.5703\n",
            "Epoch 1 Batch 200 Loss 2.4626\n",
            "Epoch 1 Batch 300 Loss 2.4660\n",
            "Epoch 2 Batch 0 Loss 2.3195\n",
            "Epoch 2 Batch 100 Loss 2.3288\n",
            "Epoch 2 Batch 200 Loss 2.2043\n",
            "Epoch 2 Batch 300 Loss 2.1493\n",
            "Epoch 3 Batch 0 Loss 2.0991\n",
            "Epoch 3 Batch 100 Loss 2.2093\n",
            "Epoch 3 Batch 200 Loss 2.1201\n",
            "Epoch 3 Batch 300 Loss 2.1164\n",
            "Epoch 4 Batch 0 Loss 1.7717\n",
            "Epoch 4 Batch 100 Loss 1.9765\n",
            "Epoch 4 Batch 200 Loss 1.7857\n",
            "Epoch 4 Batch 300 Loss 1.8709\n",
            "Epoch 5 Batch 0 Loss 1.4785\n",
            "Epoch 5 Batch 100 Loss 1.8309\n",
            "Epoch 5 Batch 200 Loss 1.6413\n",
            "Epoch 5 Batch 300 Loss 1.5224\n",
            "Epoch 6 Batch 0 Loss 1.4537\n",
            "Epoch 6 Batch 100 Loss 1.5193\n",
            "Epoch 6 Batch 200 Loss 1.5739\n",
            "Epoch 6 Batch 300 Loss 1.4957\n",
            "Epoch 7 Batch 0 Loss 1.2186\n",
            "Epoch 7 Batch 100 Loss 1.3600\n",
            "Epoch 7 Batch 200 Loss 1.3333\n",
            "Epoch 7 Batch 300 Loss 1.2933\n",
            "Epoch 8 Batch 0 Loss 1.1288\n",
            "Epoch 8 Batch 100 Loss 1.1426\n",
            "Epoch 8 Batch 200 Loss 1.1907\n",
            "Epoch 8 Batch 300 Loss 1.1583\n",
            "Epoch 9 Batch 0 Loss 0.8946\n",
            "Epoch 9 Batch 100 Loss 1.0664\n",
            "Epoch 9 Batch 200 Loss 0.9018\n",
            "Epoch 9 Batch 300 Loss 1.1046\n",
            "Epoch 10 Batch 0 Loss 0.7756\n",
            "Epoch 10 Batch 100 Loss 0.8078\n",
            "Epoch 10 Batch 200 Loss 0.8998\n",
            "Epoch 10 Batch 300 Loss 0.8592\n",
            "Epoch 11 Batch 0 Loss 0.5783\n",
            "Epoch 11 Batch 100 Loss 0.6088\n",
            "Epoch 11 Batch 200 Loss 0.6831\n",
            "Epoch 11 Batch 300 Loss 0.6680\n",
            "Epoch 12 Batch 0 Loss 0.3992\n",
            "Epoch 12 Batch 100 Loss 0.5541\n",
            "Epoch 12 Batch 200 Loss 0.4175\n",
            "Epoch 12 Batch 300 Loss 0.4689\n",
            "Epoch 13 Batch 0 Loss 0.3340\n",
            "Epoch 13 Batch 100 Loss 0.3844\n",
            "Epoch 13 Batch 200 Loss 0.3943\n",
            "Epoch 13 Batch 300 Loss 0.4218\n",
            "Epoch 14 Batch 0 Loss 0.2439\n",
            "Epoch 14 Batch 100 Loss 0.3471\n",
            "Epoch 14 Batch 200 Loss 0.3292\n",
            "Epoch 14 Batch 300 Loss 0.4539\n",
            "Epoch 15 Batch 0 Loss 0.2156\n",
            "Epoch 15 Batch 100 Loss 0.2209\n",
            "Epoch 15 Batch 200 Loss 0.2372\n",
            "Epoch 15 Batch 300 Loss 0.2182\n",
            "Epoch 16 Batch 0 Loss 0.1932\n",
            "Epoch 16 Batch 100 Loss 0.1411\n",
            "Epoch 16 Batch 200 Loss 0.1849\n",
            "Epoch 16 Batch 300 Loss 0.2330\n",
            "Epoch 17 Batch 0 Loss 0.1309\n",
            "Epoch 17 Batch 100 Loss 0.1548\n",
            "Epoch 17 Batch 200 Loss 0.1687\n",
            "Epoch 17 Batch 300 Loss 0.2455\n",
            "Epoch 18 Batch 0 Loss 0.1095\n",
            "Epoch 18 Batch 100 Loss 0.0942\n",
            "Epoch 18 Batch 200 Loss 0.1149\n",
            "Epoch 18 Batch 300 Loss 0.1196\n",
            "Epoch 19 Batch 0 Loss 0.1114\n",
            "Epoch 19 Batch 100 Loss 0.0889\n",
            "Epoch 19 Batch 200 Loss 0.1162\n",
            "Epoch 19 Batch 300 Loss 0.1079\n",
            "Epoch 20 Batch 0 Loss 0.0556\n",
            "Epoch 20 Batch 100 Loss 0.0625\n",
            "Epoch 20 Batch 200 Loss 0.0830\n",
            "Epoch 20 Batch 300 Loss 0.0761\n",
            "Epoch 21 Batch 0 Loss 0.0536\n",
            "Epoch 21 Batch 100 Loss 0.0570\n",
            "Epoch 21 Batch 200 Loss 0.1045\n",
            "Epoch 21 Batch 300 Loss 0.0736\n",
            "Epoch 22 Batch 0 Loss 0.0472\n",
            "Epoch 22 Batch 100 Loss 0.0479\n",
            "Epoch 22 Batch 200 Loss 0.0604\n",
            "Epoch 22 Batch 300 Loss 0.0568\n",
            "Epoch 23 Batch 0 Loss 0.0542\n",
            "Epoch 23 Batch 100 Loss 0.0311\n",
            "Epoch 23 Batch 200 Loss 0.0545\n",
            "Epoch 23 Batch 300 Loss 0.0481\n",
            "Epoch 24 Batch 0 Loss 0.0339\n",
            "Epoch 24 Batch 100 Loss 0.0405\n",
            "Epoch 24 Batch 200 Loss 0.0263\n",
            "Epoch 24 Batch 300 Loss 0.0554\n",
            "Epoch 25 Batch 0 Loss 0.0541\n",
            "Epoch 25 Batch 100 Loss 0.0419\n",
            "Epoch 25 Batch 200 Loss 0.0654\n",
            "Epoch 25 Batch 300 Loss 0.0451\n",
            "Epoch 26 Batch 0 Loss 0.0391\n",
            "Epoch 26 Batch 100 Loss 0.0410\n",
            "Epoch 26 Batch 200 Loss 0.0498\n",
            "Epoch 26 Batch 300 Loss 0.0705\n",
            "Epoch 27 Batch 0 Loss 0.0363\n",
            "Epoch 27 Batch 100 Loss 0.0678\n",
            "Epoch 27 Batch 200 Loss 0.0562\n",
            "Epoch 27 Batch 300 Loss 0.0482\n",
            "Epoch 28 Batch 0 Loss 0.0421\n",
            "Epoch 28 Batch 100 Loss 0.0408\n",
            "Epoch 28 Batch 200 Loss 0.0389\n",
            "Epoch 28 Batch 300 Loss 0.0500\n",
            "Epoch 29 Batch 0 Loss 0.0372\n",
            "Epoch 29 Batch 100 Loss 0.0315\n",
            "Epoch 29 Batch 200 Loss 0.0405\n",
            "Epoch 29 Batch 300 Loss 0.0307\n",
            "Epoch 30 Batch 0 Loss 0.0371\n",
            "Epoch 30 Batch 100 Loss 0.0412\n",
            "Epoch 30 Batch 200 Loss 0.0391\n",
            "Epoch 30 Batch 300 Loss 0.0542\n",
            "Epoch 31 Batch 0 Loss 0.0331\n",
            "Epoch 31 Batch 100 Loss 0.0448\n",
            "Epoch 31 Batch 200 Loss 0.0688\n",
            "Epoch 31 Batch 300 Loss 0.0424\n",
            "Epoch 32 Batch 0 Loss 0.0285\n",
            "Epoch 32 Batch 100 Loss 0.0308\n",
            "Epoch 32 Batch 200 Loss 0.0473\n",
            "Epoch 32 Batch 300 Loss 0.0410\n",
            "Epoch 33 Batch 0 Loss 0.0392\n",
            "Epoch 33 Batch 100 Loss 0.0335\n",
            "Epoch 33 Batch 200 Loss 0.0283\n",
            "Epoch 33 Batch 300 Loss 0.0251\n",
            "Epoch 34 Batch 0 Loss 0.0253\n",
            "Epoch 34 Batch 100 Loss 0.0242\n",
            "Epoch 34 Batch 200 Loss 0.0331\n",
            "Epoch 34 Batch 300 Loss 0.0215\n",
            "Epoch 35 Batch 0 Loss 0.0272\n",
            "Epoch 35 Batch 100 Loss 0.0210\n",
            "Epoch 35 Batch 200 Loss 0.0248\n",
            "Epoch 35 Batch 300 Loss 0.0197\n",
            "Epoch 36 Batch 0 Loss 0.0157\n",
            "Epoch 36 Batch 100 Loss 0.0492\n",
            "Epoch 36 Batch 200 Loss 0.0284\n",
            "Epoch 36 Batch 300 Loss 0.0507\n",
            "Epoch 37 Batch 0 Loss 0.0229\n",
            "Epoch 37 Batch 100 Loss 0.0501\n",
            "Epoch 37 Batch 200 Loss 0.0286\n",
            "Epoch 37 Batch 300 Loss 0.0425\n",
            "Epoch 38 Batch 0 Loss 0.0254\n",
            "Epoch 38 Batch 100 Loss 0.0240\n",
            "Epoch 38 Batch 200 Loss 0.0353\n",
            "Epoch 38 Batch 300 Loss 0.0426\n",
            "Epoch 39 Batch 0 Loss 0.0279\n",
            "Epoch 39 Batch 100 Loss 0.0250\n",
            "Epoch 39 Batch 200 Loss 0.0382\n",
            "Epoch 39 Batch 300 Loss 0.0360\n",
            "Epoch 40 Batch 0 Loss 0.0168\n",
            "Epoch 40 Batch 100 Loss 0.0303\n",
            "Epoch 40 Batch 200 Loss 0.0278\n",
            "Epoch 40 Batch 300 Loss 0.0194\n",
            "Epoch 41 Batch 0 Loss 0.0218\n",
            "Epoch 41 Batch 100 Loss 0.0384\n",
            "Epoch 41 Batch 200 Loss 0.0468\n",
            "Epoch 41 Batch 300 Loss 0.0206\n",
            "Epoch 42 Batch 0 Loss 0.0091\n",
            "Epoch 42 Batch 100 Loss 0.0098\n",
            "Epoch 42 Batch 200 Loss 0.0190\n",
            "Epoch 42 Batch 300 Loss 0.0228\n",
            "Epoch 43 Batch 0 Loss 0.0652\n",
            "Epoch 43 Batch 100 Loss 0.0158\n",
            "Epoch 43 Batch 200 Loss 0.0232\n",
            "Epoch 43 Batch 300 Loss 0.0410\n",
            "Epoch 44 Batch 0 Loss 0.0138\n",
            "Epoch 44 Batch 100 Loss 0.0246\n",
            "Epoch 44 Batch 200 Loss 0.0231\n",
            "Epoch 44 Batch 300 Loss 0.0296\n",
            "Epoch 45 Batch 0 Loss 0.0213\n",
            "Epoch 45 Batch 100 Loss 0.0202\n",
            "Epoch 45 Batch 200 Loss 0.0273\n",
            "Epoch 45 Batch 300 Loss 0.0228\n",
            "Epoch 46 Batch 0 Loss 0.0224\n",
            "Epoch 46 Batch 100 Loss 0.0128\n",
            "Epoch 46 Batch 200 Loss 0.0280\n",
            "Epoch 46 Batch 300 Loss 0.0415\n",
            "Epoch 47 Batch 0 Loss 0.0248\n",
            "Epoch 47 Batch 100 Loss 0.0265\n",
            "Epoch 47 Batch 200 Loss 0.0156\n",
            "Epoch 47 Batch 300 Loss 0.0238\n",
            "Epoch 48 Batch 0 Loss 0.0268\n",
            "Epoch 48 Batch 100 Loss 0.0296\n",
            "Epoch 48 Batch 200 Loss 0.0255\n",
            "Epoch 48 Batch 300 Loss 0.0177\n",
            "Epoch 49 Batch 0 Loss 0.0148\n",
            "Epoch 49 Batch 100 Loss 0.0169\n",
            "Epoch 49 Batch 200 Loss 0.0058\n",
            "Epoch 49 Batch 300 Loss 0.0225\n",
            "Epoch 50 Batch 0 Loss 0.0101\n",
            "Epoch 50 Batch 100 Loss 0.0385\n",
            "Epoch 50 Batch 200 Loss 0.0419\n",
            "Epoch 50 Batch 300 Loss 0.0387\n",
            "Epoch 50 Loss 0.0277\n",
            "Time taken for 1 epoch 33.71936273574829 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaibv3x1kE1f"
      },
      "source": [
        "def get_eval_tensor(sentence):\n",
        "    processed_sentence = preprocess_sentence(sentence)\n",
        "    try:\n",
        "        inputs = [english_tokenizer.word_index[i] for i in processed_sentence.split(' ')]\n",
        "        inputs = pad_sequences([inputs], maxlen=english_tensor.shape[1], padding='post')\n",
        "        inputs = tf.convert_to_tensor(inputs)\n",
        "        return inputs\n",
        "    except:\n",
        "        return [0 for _ in processed_sentence.split(' ')]\n",
        "        # print('The Neural Network has not learned the word yet!')\n",
        "    \n",
        "def evaluate(sentence):\n",
        "    result = ''\n",
        "    # initialize encoder hidden layer\n",
        "    hidden = [tf.zeros((1, DEC_HIDDEN_DIM))]\n",
        "    enc_out, enc_hidden = encoder(get_eval_tensor(sentence), hidden, False)\n",
        "    # False means not using dropout\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([chinese_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    attention_plot = np.zeros((chinese_tensor.shape[1], english_tensor.shape[1]))\n",
        "    \n",
        "\n",
        "    for t in range(english_tensor.shape[1]):\n",
        "        predictions, dec_hidden, attention_weights  = decoder(\n",
        "            dec_input, dec_hidden, enc_out) \n",
        "        # False means not using dropout \n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        try:\n",
        "            new_word = chinese_tokenizer.index_word[predicted_id] \n",
        "        except:\n",
        "            pass\n",
        "       \n",
        "        result += new_word + ' '\n",
        "        if chinese_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, attention_plot"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHBgmx07jz9A"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = go.Figure(data = go.Heatmap(z=attention[:-1,:],\n",
        "                                      x=sentence,\n",
        "                                      y=[i for i in predicted_sentence[:-1][::-1]]))\n",
        "    fig.update_xaxes(side=\"top\")\n",
        "    fig.update_layout(\n",
        "        autosize=False,\n",
        "        width=700,\n",
        "        height=500,\n",
        "        margin=dict(l=50, r=50, b=5, t=2, pad=4)\n",
        "        )\n",
        "    fig.show()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32XnbFUKa8o5"
      },
      "source": [
        "def translate(sentence, plot=False, score=False):\n",
        "    result, attention_plot = evaluate(sentence)\n",
        "    if score == False:\n",
        "        print(result.replace(' ', '').strip('<end>'))\n",
        "    if score:\n",
        "        return result.strip('<end> ')\n",
        "    if plot:\n",
        "        attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "        plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJpT9D5_OgP6",
        "outputId": "878aad75-e8b6-47b7-fdb2-eb29d64714c2"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fdc326a3250>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bhFfwcIMX5i"
      },
      "source": [
        "ENC_HIDDEN_DIM = 1024\n",
        "DEC_HIDDEN_DIM = 1024"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOrEqYzhNlba",
        "outputId": "6d8dd909-9618-4f21-932c-e3379305c052"
      },
      "source": [
        "translate('He is the only person I know')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "他是我所知道的人。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF-xeGRmcJFz"
      },
      "source": [
        "# Перевел как он моя единственная квартира ---"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM5BrQlbN0rh",
        "outputId": "e2195340-0768-4574-efe6-073ebae9e50b"
      },
      "source": [
        "translate('I am very hungry')\n",
        "# зачет"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很饿。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "9LF_XZ6jT16X",
        "outputId": "f4ed5ac3-bf4c-4887-a6ef-dc1dd84966cf"
      },
      "source": [
        "translate(u'The default initial hidden')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-fccb3bbea6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'The default initial hidden'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-0c0d590b40ce>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, plot, score)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-6306f42433b3>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# initialize encoder hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_HIDDEN_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_eval_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# False means not using dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-8b31e65beeea>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, init_state, training)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int32'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m   \"\"\"\n\u001b[0;32m-> 1483\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09_hUFx9EJh",
        "outputId": "da72c9e8-b624-4b3e-de7a-6bf1f60a465c"
      },
      "source": [
        "translate(u'It goes without saying')\n",
        "# don't correct"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "不言而喻。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7c5p8rmkHQG",
        "outputId": "31753c0e-cb3b-42af-cdd4-aa778c5a58ad"
      },
      "source": [
        "translate(u'We must keep books clean ')\n",
        "# don't correct"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我们必须快自己的车。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJB4hhnxGXcW"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}