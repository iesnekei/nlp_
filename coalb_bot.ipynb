{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw_9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vw0ZnImhrsh"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc24wTJmiK-I"
      },
      "source": [
        "path_to_file = 'big_text.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXJimV7LiVob",
        "outputId": "8d56b9f8-13f6-4810-8e8d-9b0035022973"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 8851733 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9FitCVNitRo",
        "outputId": "5b323e1b-46a5-4fe5-f9b7-a551d801ee29"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Старый Город. Сакраменто. Калифорния.\n",
            "Вы мистер Блэк?\n",
            "Вы меня напугали.\n",
            "Итак... я...\n",
            "Зачем вы меня позвали?\n",
            "Нужно кое что сделать.\n",
            "Всем женщинам что-то нужно. Говорите яснее.\n",
            "Конечно. Да. Извините.\n",
            "Ну, мне надо... Не знаю как и сказать...\n",
            "Просто откройте рот,\n",
            "и скажите всё вслух, \n",
            "как на исповеди.\n",
            "Вы мне о своем, я вам о своем.\n",
            "Так это делается.\n",
            "Окей.\n",
            "Я хочу, чтобы вы убили его.\n",
            "За сколько возьметесь?\n",
            "36 часов назад\n",
            "Мы считаем, что ее убили около часа назад.\n",
            "Соседи услышали крики, повонили 911.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwLA83mCi5Ua"
      },
      "source": [
        "text = text + text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KvQG2TqjFxz",
        "outputId": "8c4e4e22-10b4-4e84-93b5-c80a8ee8a0f2"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaBnCm27-hwZ",
        "outputId": "086b1fa8-85ab-4fe4-a814-23f7a3c121df"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '\\xa0', '«', '°', '»', '×', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', '\\u200b', '–', '—', '’', '“', '”', '…', '€', '№', '♪']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emTJ4stWjKby"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhCMTKrCAEyK",
        "outputId": "ba699b4f-d3ae-4fce-d505-fea2fc120f60"
      },
      "source": [
        "print(idx2char)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n' ' ' '!' '\"' '#' '$' '%' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0'\n",
            " '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '<' '=' '>' '?' '@' 'A' 'B'\n",
            " 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T'\n",
            " 'U' 'V' 'W' 'X' 'Y' 'Z' '[' '\\\\' ']' '^' '_' '`' 'a' 'b' 'c' 'd' 'e' 'f'\n",
            " 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x'\n",
            " 'y' 'z' '~' '\\xa0' '«' '°' '»' '×' 'Ё' 'А' 'Б' 'В' 'Г' 'Д' 'Е' 'Ж' 'З'\n",
            " 'И' 'Й' 'К' 'Л' 'М' 'Н' 'О' 'П' 'Р' 'С' 'Т' 'У' 'Ф' 'Х' 'Ц' 'Ч' 'Ш' 'Щ'\n",
            " 'Ы' 'Ь' 'Э' 'Ю' 'Я' 'а' 'б' 'в' 'г' 'д' 'е' 'ж' 'з' 'и' 'й' 'к' 'л' 'м'\n",
            " 'н' 'о' 'п' 'р' 'с' 'т' 'у' 'ф' 'х' 'ц' 'ч' 'ш' 'щ' 'ъ' 'ы' 'ь' 'э' 'ю'\n",
            " 'я' 'ё' '\\u200b' '–' '—' '’' '“' '”' '…' '€' '№' '♪']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Rf7PYejxo4",
        "outputId": "ab073a0a-88ca-4f2a-b910-c89f6ec693e9"
      },
      "source": [
        "text_as_int[:500]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([116, 148, 130, 146, 157, 139,   1, 102, 144, 146, 144, 134,  15,\n",
              "         1, 116, 130, 140, 146, 130, 142, 135, 143, 148, 144,  15,   1,\n",
              "       109, 130, 141, 138, 150, 144, 146, 143, 138, 161,  15,   0, 101,\n",
              "       157,   1, 142, 138, 147, 148, 135, 146,   1, 100, 141, 159, 140,\n",
              "        32,   0, 101, 157,   1, 142, 135, 143, 161,   1, 143, 130, 145,\n",
              "       149, 133, 130, 141, 138,  15,   0, 107, 148, 130, 140,  15,  15,\n",
              "        15,   1, 161,  15,  15,  15,   0, 106, 130, 153, 135, 142,   1,\n",
              "       132, 157,   1, 142, 135, 143, 161,   1, 145, 144, 137, 132, 130,\n",
              "       141, 138,  32,   0, 112, 149, 136, 143, 144,   1, 140, 144, 135,\n",
              "         1, 153, 148, 144,   1, 147, 134, 135, 141, 130, 148, 158,  15,\n",
              "         0, 101, 147, 135, 142,   1, 136, 135, 143, 155, 138, 143, 130,\n",
              "       142,   1, 153, 148, 144,  14, 148, 144,   1, 143, 149, 136, 143,\n",
              "       144,  15,   1, 102, 144, 132, 144, 146, 138, 148, 135,   1, 161,\n",
              "       147, 143, 135, 135,  15,   0, 109, 144, 143, 135, 153, 143, 144,\n",
              "        15,   1, 103, 130,  15,   1, 107, 137, 132, 138, 143, 138, 148,\n",
              "       135,  15,   0, 112, 149,  13,   1, 142, 143, 135,   1, 143, 130,\n",
              "       134, 144,  15,  15,  15,   1, 112, 135,   1, 137, 143, 130, 160,\n",
              "         1, 140, 130, 140,   1, 138,   1, 147, 140, 130, 137, 130, 148,\n",
              "       158,  15,  15,  15,   0, 114, 146, 144, 147, 148, 144,   1, 144,\n",
              "       148, 140, 146, 144, 139, 148, 135,   1, 146, 144, 148,  13,   0,\n",
              "       138,   1, 147, 140, 130, 136, 138, 148, 135,   1, 132, 147, 162,\n",
              "         1, 132, 147, 141, 149, 151,  13,   1,   0, 140, 130, 140,   1,\n",
              "       143, 130,   1, 138, 147, 145, 144, 132, 135, 134, 138,  15,   0,\n",
              "       101, 157,   1, 142, 143, 135,   1, 144,   1, 147, 132, 144, 135,\n",
              "       142,  13,   1, 161,   1, 132, 130, 142,   1, 144,   1, 147, 132,\n",
              "       144, 135, 142,  15,   0, 117, 130, 140,   1, 159, 148, 144,   1,\n",
              "       134, 135, 141, 130, 135, 148, 147, 161,  15,   0, 113, 140, 135,\n",
              "       139,  15,   0, 129,   1, 151, 144, 153, 149,  13,   1, 153, 148,\n",
              "       144, 131, 157,   1, 132, 157,   1, 149, 131, 138, 141, 138,   1,\n",
              "       135, 133, 144,  15,   0, 106, 130,   1, 147, 140, 144, 141, 158,\n",
              "       140, 144,   1, 132, 144, 137, 158, 142, 135, 148, 135, 147, 158,\n",
              "        32,   0,  20,  23,   1, 153, 130, 147, 144, 132,   1, 143, 130,\n",
              "       137, 130, 134,   0, 111, 157,   1, 147, 153, 138, 148, 130, 135,\n",
              "       142,  13,   1, 153, 148, 144,   1, 135, 135,   1, 149, 131, 138,\n",
              "       141, 138,   1, 144, 140, 144, 141, 144,   1, 153, 130, 147, 130,\n",
              "         1, 143, 130, 137, 130, 134,  15,   0, 116, 144, 147, 135, 134,\n",
              "       138,   1, 149, 147, 141, 157, 154, 130, 141, 138,   1, 140, 146,\n",
              "       138, 140, 138,  13,   1, 145, 144, 132, 144, 143, 138, 141, 138,\n",
              "         1,  26,  18,  18,  15,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxdPNouaj02-"
      },
      "source": [
        "text_as_int, text, len(text_as_int), len(text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvVeVpmtj-sO"
      },
      "source": [
        "# TRAIN AND TARGET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5ym-hr_j4eu",
        "outputId": "3586e636-6020-44ec-c0e9-0850a36b18bd"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "С\n",
            "т\n",
            "а\n",
            "р\n",
            "ы\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g69cDpkFkMyv",
        "outputId": "c2e6abb7-aeb8-43c4-cbb6-94c7587518db"
      },
      "source": [
        "type(char_dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xoPaf1pkQ30",
        "outputId": "cd11820d-565a-435a-946b-756277154cfd"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Старый Город. Сакраменто. Калифорния.\\nВы мистер Блэк?\\nВы меня напугали.\\nИтак... я...\\nЗачем вы меня по'\n",
            "'звали?\\nНужно кое что сделать.\\nВсем женщинам что-то нужно. Говорите яснее.\\nКонечно. Да. Извините.\\nНу, '\n",
            "'мне надо... Не знаю как и сказать...\\nПросто откройте рот,\\nи скажите всё вслух, \\nкак на исповеди.\\nВы м'\n",
            "'не о своем, я вам о своем.\\nТак это делается.\\nОкей.\\nЯ хочу, чтобы вы убили его.\\nЗа сколько возьметесь?'\n",
            "'\\n36 часов назад\\nМы считаем, что ее убили около часа назад.\\nСоседи услышали крики, повонили 911.\\nСвиде'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN1KvO55kx5i",
        "outputId": "4f71e4f0-e284-4b43-b5c5-522ca21a0ffd"
      },
      "source": [
        "type(sequences)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.BatchDataset"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvDw6qZlPTw"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ2jnT0LlmMw"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoQcmx_Gmbf_",
        "outputId": "1b973071-4607-4f5f-a058-0bdb167f25f6"
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_VXpUdtmcWw",
        "outputId": "4203256a-63d9-4614-a77a-9219989267d3"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'Старый Город. Сакраменто. Калифорния.\\nВы мистер Блэк?\\nВы меня напугали.\\nИтак... я...\\nЗачем вы меня п'\n",
            "Target data: 'тарый Город. Сакраменто. Калифорния.\\nВы мистер Блэк?\\nВы меня напугали.\\nИтак... я...\\nЗачем вы меня по'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vk60KdLm6cv",
        "outputId": "73217b26-2b6d-4804-f9f9-752bab5842f9"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVonyRPvnavf"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIBMJd0Gnkma"
      },
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eu8FpUNnuEE"
      },
      "source": [
        "class RNNgenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, batch_size):\n",
        "        super(RNNgenerator, self).__init__()\n",
        "        \n",
        "        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "                                 \n",
        "        self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform')\n",
        "        self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform')\n",
        "                           \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        emb_x = self.emb(x)\n",
        "        x1 = self.gru1(emb_x)\n",
        "        x = x1\n",
        "        for _ in range(3):\n",
        "            x = self.gru2(x)\n",
        "        #x = self.gru1(x)\n",
        "        x = (x + x1)/2\n",
        "        return self.fc(x)\n",
        "\n",
        "model = RNNgenerator(vocab_size, embedding_dim, BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Xpfo5yoa4h"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "                                 \n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "         tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        \n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "                                   \n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CSerjWAogXx"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ADoiFZPomo-",
        "outputId": "fb88827e-62f6-4744-904c-d230be1006a3"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 173) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlpcl8biosoO",
        "outputId": "158844ac-11a9-4ad2-d9c2-95707390855a"
      },
      "source": [
        "example_batch_predictions[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 173), dtype=float32, numpy=\n",
              "array([[ 1.7961696e-06,  3.4138714e-06,  9.9950057e-06, ...,\n",
              "        -5.5940468e-06, -1.0042542e-05,  2.9910629e-07],\n",
              "       [ 1.2632729e-05,  2.5195386e-05,  1.4866309e-05, ...,\n",
              "        -1.7909821e-05, -3.7675603e-05,  1.0204304e-05],\n",
              "       [ 3.1122898e-05,  5.0838524e-05,  2.9241683e-06, ...,\n",
              "        -4.8101610e-05, -8.4514584e-05,  3.6313715e-05],\n",
              "       ...,\n",
              "       [-1.2971531e-04, -3.0032765e-05, -4.4984707e-05, ...,\n",
              "         1.6919721e-03, -3.6038589e-04, -1.0543493e-04],\n",
              "       [-1.7698699e-04, -4.4954424e-05,  8.6238477e-05, ...,\n",
              "         1.7523851e-03, -2.4058612e-04, -1.7173105e-04],\n",
              "       [-2.5359663e-04, -5.4225955e-05,  2.2647035e-04, ...,\n",
              "         1.8202304e-03, -1.2649239e-04, -2.3768030e-04]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC0I1BO6o1xn"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGJ8np6Bo8iX",
        "outputId": "71e2d595-85cd-4e76-b5ef-83ba7cafe434"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " 'м положении.\\nЕще одно дело.\\nПристегнись.\\nВан Пелт, ты со мной.\\nВы, ребята, поговорите со специалиста'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"♪E——-F–вфeЯ1I'e»ЛюЩА—[=цso«юа4дfAЯ<х»П\\n!LZeЮЯPlj?в:r3s-*Ц\\ntЕвqM+\\\\ФQ…УрRж–АKмZ4юЫх$-l—ЩF&Уhp'П,с5Vё(H\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ_MXh4Qo_Au",
        "outputId": "63c3c6dc-ae94-4e6e-ec6b-b2f2e172f35a"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 173)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       5.153638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12rqHe7bpH_G"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WONuc9jPpLjM"
      },
      "source": [
        "!rm -rf ./training_checkpoints"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dykk4iw0pORt",
        "outputId": "20973721-ceac-4db3-f15a-864320a16aa8"
      },
      "source": [
        "!ls ./training_checkpoints"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './training_checkpoints': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEo8mwtapPsv"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_freq=88*3,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsh-KqJMpXYY"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83xkMWBwpY4V",
        "outputId": "c532aeaf-74af-45f7-e56c-60be45121998"
      },
      "source": [
        "with tf.device(\"/gpu:0\"):\n",
        "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2738/2738 [==============================] - 656s 238ms/step - loss: 1.6196\n",
            "Epoch 2/20\n",
            "2738/2738 [==============================] - 652s 238ms/step - loss: 1.2333\n",
            "Epoch 3/20\n",
            "2738/2738 [==============================] - 654s 238ms/step - loss: 1.1518\n",
            "Epoch 4/20\n",
            "2738/2738 [==============================] - 653s 238ms/step - loss: 1.0932\n",
            "Epoch 5/20\n",
            "2738/2738 [==============================] - 654s 238ms/step - loss: 1.0463\n",
            "Epoch 6/20\n",
            "2738/2738 [==============================] - 654s 239ms/step - loss: 1.0084\n",
            "Epoch 7/20\n",
            "2738/2738 [==============================] - 654s 238ms/step - loss: 0.9783\n",
            "Epoch 8/20\n",
            "2738/2738 [==============================] - 654s 238ms/step - loss: 0.9550\n",
            "Epoch 9/20\n",
            "2738/2738 [==============================] - 654s 239ms/step - loss: 0.9376\n",
            "Epoch 10/20\n",
            "2738/2738 [==============================] - 654s 238ms/step - loss: 0.9252\n",
            "Epoch 11/20\n",
            "2738/2738 [==============================] - 654s 239ms/step - loss: 0.9165\n",
            "Epoch 12/20\n",
            "2738/2738 [==============================] - 662s 242ms/step - loss: 0.9109\n",
            "Epoch 13/20\n",
            "2738/2738 [==============================] - 665s 242ms/step - loss: 0.9081\n",
            "Epoch 14/20\n",
            "2738/2738 [==============================] - 666s 243ms/step - loss: 0.9076\n",
            "Epoch 15/20\n",
            "2738/2738 [==============================] - 665s 243ms/step - loss: 0.9088\n",
            "Epoch 16/20\n",
            "2738/2738 [==============================] - 665s 243ms/step - loss: 0.9117\n",
            "Epoch 17/20\n",
            "2738/2738 [==============================] - 666s 243ms/step - loss: 0.9145\n",
            "Epoch 18/20\n",
            "2738/2738 [==============================] - 665s 243ms/step - loss: 0.9191\n",
            "Epoch 19/20\n",
            "2738/2738 [==============================] - 666s 243ms/step - loss: 0.9255\n",
            "Epoch 20/20\n",
            "2738/2738 [==============================] - 666s 243ms/step - loss: 0.9314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk-AR-Xgto6o"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU-In6ANttpJ"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO6ukuMFt_cq",
        "outputId": "a49199de-13d6-4bac-d46a-ebf3beec80d2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 128)            22144     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (1, None, 1024)           4722688   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 173)            177325    \n",
            "=================================================================\n",
            "Total params: 30,100,269\n",
            "Trainable params: 30,100,269\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u15vR1CFuBiv"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 500\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.5\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi4SKJw4uFFU",
        "outputId": "95a6ef07-4f64-4b44-d737-f3805ad07800"
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"сижу я на стуле \")\n",
        "print(text_)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "сижу я на стуле - это так тяжело расследование,\n",
            "они не продавали наркотики.\n",
            "Это может быть совпадением.\n",
            "У неё были наркотики, о которых они не знают.\n",
            "Простите.\n",
            "Как вы узнали, что это было?\n",
            "Я не знаю. Может быть.\n",
            "Вы думаете, что это они сделали?\n",
            "Вы убили Даррела Гонзалеза.\n",
            "Что ж, это будет невероятно.\n",
            "Хотя и нападение на офицера полиции Сакраменто.\n",
            "Конечно, он как-то не похож на деньги.\n",
            "Его последняя война.\n",
            "Наша фальшивая старая привыкла к насилию?\n",
            "Бывает, да?\n",
            "Это для тебя.\n",
            "Я знаю, что делать.\n",
            "Но будь рядом.\n",
            "Ита\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExGzp6fNMAcs",
        "outputId": "33d2d393-6782-421d-fe37-edc45ac4aa0d"
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"Как Дела? \")\n",
        "print(text_)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Как Дела? Как вы смеете?!\n",
            "Он говорит правду, об этом не согласитесь.\n",
            "Ты выглядишь немного странно.\n",
            "Я не думаю, что открыл сейф.\n",
            "Точно.\n",
            "Я не знаю, что ты скажешь, правда?\n",
            "Мы поедем в ее пьяном виду.\n",
            "Не представляю, как все равно.\n",
            "Послушай, ты меня поняла?\n",
            "Они оставили сообщение по сосновому поговорить с тобой.\n",
            "Что ты делаешь?\n",
            "Ой не могу.\n",
            "Я не могу поделить деньги на стороне.\n",
            "Ты знаешь, что тогда было...\n",
            "Это было довольно виновным.\n",
            "Ты не думал, что это правда.\n",
            "Даже не спрашивай правду.\n",
            "Я сказала ей уйти.\n",
            "Ты\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv-amXqgjyiN"
      },
      "source": [
        "\n",
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 500\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.45\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (''.join(text_generated))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdOcs5bKj--T",
        "outputId": "bd68ccbb-6d9b-4ff8-d4c3-050df865d6b0"
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"кто вы? \")\n",
        "print(text_)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Какой ещё друг?\n",
            "Просто поговорим.\n",
            "Поговори с Ригсби.\n",
            "Ригсби, проверь камеры наблюдения.\n",
            "Да, босс.\n",
            "Нет, нет. Дух звонили.\n",
            "Слушай, я знаю, что вы сделали.\n",
            "Спасибо.\n",
            "Вы можете присоединиться к нам.\n",
            "Да, мы поняли, когда он начал разговаривать.\n",
            "- Мы должны держаться в стороне.\n",
            "- Почему вы так долго?\n",
            "— Да, я в порядке.\n",
            "Вы сказали, что вы думаете о моей матери.\n",
            "- Не видели Джереми?\n",
            "Он был здесь вчера вечером. Это было нечто не угодно.\n",
            "И вы пошли после убийства.\n",
            "Вы забрали его после того, как вы скажете \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfzSslATkAbi",
        "outputId": "479719b8-c52c-4079-a37a-8bb23a737c71"
      },
      "source": [
        "!zip -r 'training_checkpoints.zip' 'training_checkpoints'"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: training_checkpoints/ (stored 0%)\n",
            "updating: training_checkpoints/ckpt_18.index (deflated 72%)\n",
            "updating: training_checkpoints/ckpt_12.index (deflated 72%)\n",
            "updating: training_checkpoints/ckpt_2.data-00000-of-00001\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcGM0JZLmZKQ",
        "outputId": "496e7ff9-8d6a-4d14-bf42-aa428eaaff72"
      },
      "source": [
        "model.save(\"my_model\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: my_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: my_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Ofjj9zwNSw",
        "outputId": "edd299be-e4eb-4fb8-c8ee-c067f3a1e5ef"
      },
      "source": [
        "reconstructed_model = tf.keras.models.load_model(\"my_model\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-htkNs2-wqaN"
      },
      "source": [
        "reconstructed_model.compile()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhNXoAcUwuUV",
        "outputId": "103611ca-32f9-4d34-9c77-515918d2e1ff"
      },
      "source": [
        "text_ = generate_text(reconstructed_model, start_string=u\"кто вы? \")\n",
        "print(text_)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "кто вы? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC0sItKHw2S-",
        "outputId": "1052cdcc-885b-4c94-c842-4afe15bbd454"
      },
      "source": [
        "!zip -r 'my_model.zip' 'my_model'"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: my_model/ (stored 0%)\n",
            "  adding: my_model/variables/ (stored 0%)\n",
            "  adding: my_model/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: my_model/variables/variables.index (deflated 64%)\n",
            "  adding: my_model/saved_model.pb (deflated 90%)\n",
            "  adding: my_model/keras_metadata.pb (deflated 93%)\n",
            "  adding: my_model/assets/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVvdIGdfw_v7",
        "outputId": "d191525a-aca7-425a-95dd-a56f733463be"
      },
      "source": [
        "print(text_)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "кто вы? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYUeAw9qSXgp",
        "outputId": "31d96833-a432-403d-ca9c-39c0110d1f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"кто вы? \")\n",
        "print(text_)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['К', 'а', 'к', 'о', 'й', ' ', 'е', 'щ', 'е', ' ', 'д', 'р', 'у', 'г', ',', ' ', 'м', 'ы', ' ', 'н', 'е', ' ', 'м', 'о', 'ж', 'е', 'м', ' ', 'п', 'о', 'з', 'в', 'о', 'л', 'и', 'т', 'ь', ' ', 'н', 'а', 'м', ' ', 'п', 'р', 'и', 'с', 'о', 'е', 'д', 'и', 'н', 'и', 'т', 'ь', 'с', 'я', ' ', 'в', ' ', 'л', 'ю', 'б', 'о', 'е', ' ', 'в', 'р', 'е', 'м', 'я', '.', '\\n', 'П', 'о', 'с', 'л', 'у', 'ш', 'а', 'й', 'т', 'е', ',', ' ', 'п', 'а', 'р', 'е', 'н', 'ь', ',', ' ', 'к', 'о', 'н', 'е', 'ч', 'н', 'о', ',', ' ', 'н', 'о', ' ', 'м', 'ы', ' ', 'н', 'е', ' ', 'з', 'н', 'а', 'е', 'м', '.', '\\n', 'П', 'о', 'г', 'о', 'в', 'о', 'р', 'и', 'т', 'е', ' ', 'с', ' ', 'о', 'д', 'н', 'и', 'м', ' ', 'и', 'з', ' ', 'п', 'о', 'п', 'у', 'л', 'я', 'р', 'н', 'ы', 'х', ' ', 'р', 'а', 'с', 'с', 'л', 'е', 'д', 'о', 'в', 'а', 'н', 'и', 'й', '.', '\\n', '-', ' ', 'О', 'н', ' ', 'б', 'ы', 'л', ' ', 'о', 'д', 'и', 'н', ' ', 'и', 'з', ' ', 'н', 'и', 'х', '.', '\\n', '-', ' ', 'В', 'ы', ' ', 'з', 'н', 'а', 'е', 'т', 'е', ',', '\\n', 'ч', 'т', 'о', ' ', 'о', 'н', ' ', 'п', 'е', 'р', 'е', 'о', 'д', 'е', 'л', 'с', 'я', ' ', 'в', ' ', 'т', 'о', 'т', ' ', 'р', 'а', 'з', ',', '\\n', 'ж', 'и', 'в', 'ы', ' ', 'б', 'ы', 'л', 'и', ' ', 'о', 'ч', 'е', 'н', 'ь', ' ', 'б', 'л', 'и', 'з', 'к', 'и', '.', '\\n', 'К', 'а', 'к', ' ', 'т', 'о', 'л', 'ь', 'к', 'о', ' ', 'о', 'н', 'а', ' ', 'у', 'е', 'х', 'а', 'л', 'а', ',', ' ', 'в', 'ы', ' ', 'с', 'о', 'в', 'е', 'р', 'ш', 'и', 'л', 'и', ' ', 'у', 'б', 'и', 'й', 'с', 'т', 'в', 'о', '.', '\\n', 'П', 'о', 'ч', 'е', 'м', 'у', ' ', 'в', 'ы', ' ', 'х', 'о', 'т', 'и', 'т', 'е', ' ', 'п', 'о', 'м', 'о', 'ч', 'ь', ' ', 'м', 'н', 'е', '?', '\\n', 'В', 'ы', ' ', 'м', 'о', 'ж', 'е', 'т', 'е', ' ', 'н', 'а', 'ч', 'а', 'т', 'ь', ' ', 'р', 'а', 'б', 'о', 'т', 'у', ' ', 'п', 'о', ' ', 'м', 'а', 'с', 'л', 'у', '\\n', 'и', ' ', 'н', 'е', ' ', 'с', 'к', 'а', 'ж', 'е', 'т', 'е', ',', ' ', 'ч', 'т', 'о', ' ', 'в', 'ы', ' ', 'с', 'д', 'е', 'л', 'а', 'л', 'и', '?', '\\n', 'Я', ' ', 'с', 'д', 'е', 'л', 'а', 'л', 'а', ' ', 'т', 'о', ',', ' ', 'ч', 'т', 'о', ' ', 'с', 'о', 'б', 'и', 'р', 'а', 'л', 'а', 'с', 'ь', ' ', 'д', 'е', 'л', 'а', 'т', 'ь', '.', '\\n', 'Я', ' ', 'н', 'е', ' ', 'м', 'о', 'г', 'у', ' ', 'с', 'к', 'а', 'з', 'а', 'т', 'ь', ' ', 'в', 'а', 'м', '.', '\\n', 'Я', ' ', 'н', 'е', ' ', 'м', 'о', 'г', 'у', ' ', 'п', 'е', 'р', 'е', 'д', 'а', 'т', 'ь', ' ', 'э', 'т', 'и', ' ', 'в', 'о', 'п', 'р', 'о', 'с', 'ы', '.', '\\n', 'И', 'т', 'а', 'к', ',', ' ', 'м', 'и', 'с', 'т', 'е', 'р', ' ', 'Д', 'ж', 'е', 'й']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk07WENpSgb0",
        "outputId": "93faf885-b7b2-4f01-9dbc-d384197d512a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install telebot"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting telebot\n",
            "  Downloading telebot-0.0.4-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from telebot) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->telebot) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->telebot) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->telebot) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->telebot) (2021.5.30)\n",
            "Installing collected packages: telebot\n",
            "Successfully installed telebot-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcApsYasTEHp",
        "outputId": "2691ff08-83af-4ca2-9b93-ed0077025ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyTelegramBotAPI"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyTelegramBotAPI\n",
            "  Downloading pyTelegramBotAPI-4.1.1.tar.gz (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 92 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 102 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyTelegramBotAPI) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (2021.5.30)\n",
            "Building wheels for collected packages: pyTelegramBotAPI\n",
            "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyTelegramBotAPI: filename=pyTelegramBotAPI-4.1.1-py3-none-any.whl size=80779 sha256=d562c7758bfa27bbfeb6d404f34d687a503c2c3e0e511a270f3562e918f9a667\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/9d/f5/f589ebef11a6541a4e5a1793a380db577bab5583eac6b45514\n",
            "Successfully built pyTelegramBotAPI\n",
            "Installing collected packages: pyTelegramBotAPI\n",
            "Successfully installed pyTelegramBotAPI-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9yKPFlSTcK8",
        "outputId": "b934c6d6-f5cd-49ec-9742-674863c56ef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"кто вы? \")\n",
        "print(text_)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Как ваше имя?\n",
            "Извините. Вам нужно поговорить с вами.\n",
            "Вы не можете этого сделать.\n",
            "- Понял.\n",
            "Понял.\n",
            "Что вы делаете?\n",
            "Он был в Сан-Франциско не был в беде, не так ли?\n",
            "Нет, не думаю.\n",
            "Вы знаете, что вы делаете?\n",
            "Он покинул страну прошлой ночью.\n",
            "Ваш контакт с кем-то спал на вас.\n",
            "Может, вы услышали о похищении в нескольких кварталах\n",
            "от дома Кузьменко,\n",
            "не представляли ваше возможное остальные.\n",
            "Вы не можете их запомнить.\n",
            "Простите. Я не знаю, что вы скажете.\n",
            "Хорошо. Я посмотрю, что можно.\n",
            "Теперь мы можем пог\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-qvW27T75L",
        "outputId": "327443ab-8f44-4d38-9d7a-c2bfaa97f449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "text_.split('\\n')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Какой ещё друг?',\n",
              " 'Просто поговорим.',\n",
              " 'Поговори с Ригсби.',\n",
              " 'Ригсби, проверь камеры наблюдения.',\n",
              " 'Да, босс.',\n",
              " 'Нет, нет. Дух звонили.',\n",
              " 'Слушай, я знаю, что вы сделали.',\n",
              " 'Спасибо.',\n",
              " 'Вы можете присоединиться к нам.',\n",
              " 'Да, мы поняли, когда он начал разговаривать.',\n",
              " '- Мы должны держаться в стороне.',\n",
              " '- Почему вы так долго?',\n",
              " '— Да, я в порядке.',\n",
              " 'Вы сказали, что вы думаете о моей матери.',\n",
              " '- Не видели Джереми?',\n",
              " 'Он был здесь вчера вечером. Это было нечто не угодно.',\n",
              " 'И вы пошли после убийства.',\n",
              " 'Вы забрали его после того, как вы скажете ']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqilTw-uVLJN",
        "outputId": "e11974fb-674c-4e15-bb03-5bedf2d8d694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "random.choice(text_.split('\\n'))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'— Да, я в порядке.'"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzWFzlEtXJyo"
      },
      "source": [
        "TOKEN = '1941575671:AAEtokLdPmZxIMTY644hXc0Wj-y-uSBco_Y'\n",
        "import telebot\n",
        "from telebot import types\n",
        "\n",
        "bot = telebot.TeleBot(TOKEN)\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def welcome(message):\n",
        "    sti = open('static/welcome.webp', 'rb')\n",
        "    bot.send_sticker(message.chat.id, sti)\n",
        "\n",
        "    # keyboard\n",
        "    # markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
        "    # item1 = types.KeyboardButton(\"🎲 Рандомное число\")\n",
        "    # item2 = types.KeyboardButton(\"😊 Как дела?\")\n",
        "    #\n",
        "    # markup.add(item1, item2)\n",
        "\n",
        "    bot.send_message(message.chat.id,\n",
        "                     \"Добро пожаловать, {0.first_name}!\\nЯ - <b>{1.first_name}</b>,\"\n",
        "                     \" бот созданный чтобы быть переводчиком китайский-русский.\".format(\n",
        "                         message.from_user, bot.get_me()),\n",
        "                     parse_mode='html',) # reply_markup=markup)\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def lalala(message):\n",
        "    if message.chat.type == 'private':\n",
        "        if message.text == '🎲 Рандомное число':\n",
        "            bot.send_message(message.chat.id, str(random.randint(0, 100)))\n",
        "        elif message.text == '😊 Как дела?':\n",
        "\n",
        "            markup = types.InlineKeyboardMarkup(row_width=2)\n",
        "            item1 = types.InlineKeyboardButton(\"Хорошо\", callback_data='good')\n",
        "            item2 = types.InlineKeyboardButton(\"Не очень\", callback_data='bad')\n",
        "\n",
        "            markup.add(item1, item2)\n",
        "\n",
        "            bot.send_message(message.chat.id, 'Отлично, сам как?', reply_markup=markup)\n",
        "        elif message.text:\n",
        "            # url = get_url(message.text)\n",
        "            # soup = get_soup(url)\n",
        "            # words = translate(soup)\n",
        "            # for i in words:\n",
        "            #     bot.send_message(message.chat.id, i)\n",
        "            bot.send_message(message.chat.id, random.choice(generate_text(model, start_string=message.text).split('\\n')[:-1]))\n",
        "\n",
        "\n",
        "        else:\n",
        "            bot.send_message(message.chat.id, 'Я не знаю что ответить 😢')\n",
        "\n",
        "\n",
        "@bot.callback_query_handler(func=lambda call: True)\n",
        "def callback_inline(call):\n",
        "    try:\n",
        "        if call.message:\n",
        "            if call.data == 'good':\n",
        "                bot.send_message(call.message.chat.id, 'Вот и отличненько 😊')\n",
        "            elif call.data == 'bad':\n",
        "                bot.send_message(call.message.chat.id, 'Бывает 😢')\n",
        "\n",
        "            # remove inline buttons\n",
        "            bot.edit_message_text(chat_id=call.message.chat.id, message_id=call.message.message_id, text=\"😊 Как дела?\",\n",
        "                                  reply_markup=None)\n",
        "\n",
        "            # show alert\n",
        "            bot.answer_callback_query(callback_query_id=call.id, show_alert=False,\n",
        "                                      text=\"ЭТО ТЕСТОВОЕ УВЕДОМЛЕНИЕ!!11\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(repr(e))\n",
        "\n",
        "\n",
        "# RUN\n",
        "bot.polling(none_stop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJUCVMLydVxv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}