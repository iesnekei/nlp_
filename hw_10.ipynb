{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "# 3. Бибилотека чтобы разделять грамотно иероглифы\n",
        "import jieba"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNvjhDyAKk3U",
        "outputId": "41c38344-99f7-4b03-dd05-b3e7a5de82b4"
      },
      "source": [
        "!wget http://www.manythings.org/anki/cmn-eng.zip"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-06 07:14:03--  http://www.manythings.org/anki/cmn-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3033::ac43:ba36, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1153006 (1.1M) [application/zip]\n",
            "Saving to: ‘cmn-eng.zip.2’\n",
            "\n",
            "\rcmn-eng.zip.2         0%[                    ]       0  --.-KB/s               \rcmn-eng.zip.2       100%[===================>]   1.10M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-10-06 07:14:03 (19.4 MB/s) - ‘cmn-eng.zip.2’ saved [1153006/1153006]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bg17Lr-7XK",
        "outputId": "7e2e4349-972b-4060-cd72-a2d2b3ded8af"
      },
      "source": [
        "!mkdir cn-eng\n",
        "!unzip cmn-eng.zip -d cn-eng/"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘cn-eng’: File exists\n",
            "Archive:  cmn-eng.zip\n",
            "replace cn-eng/cmn.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: cn-eng/cmn.txt          \n",
            "  inflating: cn-eng/_about.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o5L92efMMhf",
        "outputId": "252cde9b-e44a-4c18-ece3-4f50a0f0a10f"
      },
      "source": [
        "\n",
        "!ls /content/cn-eng/ -lah"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.7M\n",
            "drwxr-xr-x 2 root root 4.0K Oct  6 07:14 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct  6 07:14 ..\n",
            "-rw-r--r-- 1 root root 1.5K Jul 14 10:16 _about.txt\n",
            "-rw-r--r-- 1 root root 3.6M Jul 14 10:16 cmn.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "source": [
        "# Download the file\n",
        "path_to_file = \"/content/cn-eng/cmn.txt\""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "source": [
        "# 2. Потом я заметил что модель выплевывает пустые строки, думаю что надо разделять иероглифы каждый между собой. Добавим процедуру cutword и unicode_to_ascii в функцию preprocess_sentence\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "    \n",
        "def cutword(sentence):\n",
        "    output = []\n",
        "    for word in jieba.cut(sentence, cut_all=False):\n",
        "        output.append(word)\n",
        "    output = ' '.join(output)\n",
        "    return output\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.。!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,？。！，\\u4e00-\\u9FFF]+\", \" \", w)\n",
        "  if len(re.findall('([a-z])',w)) == 0:\n",
        "        try:\n",
        "            w = cutword(w)\n",
        "        except:\n",
        "            pass\n",
        "  \n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfGQ7c-_cKT4"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# cut chinese\n",
        "def cutword(sentence):\n",
        "    output = []\n",
        "    for word in jieba.cut(sentence, cut_all=False):\n",
        "        output.append(word)\n",
        "    output = ' '.join(output)\n",
        "    return output\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    w = re.sub(r\"([?.!,？。！，])\", r\" \\1 \", w)\n",
        "    # delete extra spaces\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    # as well as Chinese characters\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,？。！，\\u4e00-\\u9FFF]+\", \" \", w)\n",
        "\n",
        "    # cut words\n",
        "    if len(re.findall('([a-z])',w)) == 0:\n",
        "        try:\n",
        "            w = cutword(w)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "yV9lZXQXNbnH",
        "outputId": "1c7c3582-290a-4a5c-ccf1-3287f34339b4"
      },
      "source": [
        "cutword(\"我是你大爷。\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'我 是 你 大爷 。'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm0OXZ5hcQyA",
        "outputId": "8f22d65d-fa43-47cc-b85a-273d0724f1cb"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "zh_sentence = u\"我可以借这本书吗？\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(zh_sentence))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> may i borrow this book ?  <end>\n",
            "<start> 我 可以 借 这 本书 吗   ？   <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ3OrQ6anwIZ"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = []\n",
        "    for line in lines[:num_examples]:\n",
        "        sentences = line.split('\\t')[:2]\n",
        "        word_pairs.append((preprocess_sentence(sentences[0]),\n",
        "                           preprocess_sentence(sentences[1])))\n",
        "    return word_pairs"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIuJzXd7oEyq",
        "outputId": "48e6dbdd-8bd1-447f-cd12-c5e71c64033e"
      },
      "source": [
        "for l,k in create_dataset(path_to_file, 10):\n",
        "    print(l)\n",
        "    print(k)\n",
        "    print('English length: {}; Chinese length: {}'.format(len(l),len(k)))\n",
        "    print('--' *30)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> hi .  <end>\n",
            "<start> 嗨   。   <end>\n",
            "English length: 19; Chinese length: 21\n",
            "------------------------------------------------------------\n",
            "<start> hi .  <end>\n",
            "<start> 你好   。   <end>\n",
            "English length: 19; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> run .  <end>\n",
            "<start> 你 用 跑 的   。   <end>\n",
            "English length: 20; Chinese length: 27\n",
            "------------------------------------------------------------\n",
            "<start> wait !  <end>\n",
            "<start> 等等   ！   <end>\n",
            "English length: 21; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> wait !  <end>\n",
            "<start> 等 一下   ！   <end>\n",
            "English length: 21; Chinese length: 24\n",
            "------------------------------------------------------------\n",
            "<start> begin .  <end>\n",
            "<start> 开始   ！   <end>\n",
            "English length: 22; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> hello !  <end>\n",
            "<start> 你好   。   <end>\n",
            "English length: 22; Chinese length: 22\n",
            "------------------------------------------------------------\n",
            "<start> i try .  <end>\n",
            "<start> 我 试试   。   <end>\n",
            "English length: 22; Chinese length: 24\n",
            "------------------------------------------------------------\n",
            "<start> i won !  <end>\n",
            "<start> 我 赢 了   。   <end>\n",
            "English length: 22; Chinese length: 25\n",
            "------------------------------------------------------------\n",
            "<start> oh no !  <end>\n",
            "<start> 不会 吧   。   <end>\n",
            "English length: 22; Chinese length: 24\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFR6khs6o1vK",
        "outputId": "229d09ed-77da-440c-c07f-419bad8ea77a"
      },
      "source": [
        "english = []\n",
        "chinese = []\n",
        "\n",
        "for en, zh in create_dataset(path_to_file, None):\n",
        "    english.append(en)\n",
        "    chinese.append(zh)\n",
        "\n",
        "print('--------- Original ---------')\n",
        "with open (path_to_file) as f:\n",
        "    for i in (f.read().split('\\n')[-2].split('\\t')):\n",
        "        print(i)\n",
        "print('')\n",
        "print('--------- Processed ---------')\n",
        "print(english[-1])\n",
        "print(chinese[-1])\n",
        "print('')\n",
        "print('Number of English Sentences: ', len(english))\n",
        "print('Number of Chinese sentences: ', len(chinese))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- Original ---------\n",
            "If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\n",
            "如果一個人在成人前沒有機會習得目標語言，他對該語言的認識達到母語者程度的機會是相當小的。\n",
            "CC-BY 2.0 (France) Attribution: tatoeba.org #1230633 (alec) & #1205914 (cienias)\n",
            "\n",
            "--------- Processed ---------\n",
            "<start> if a person has not had a chance to acquire his target language by the time he s an adult , he s unlikely to be able to reach native speaker level in that language .  <end>\n",
            "<start> 如果 一個 人 在 成人 前 沒 有 機會習 得 目標 語言   ，   他 對 該 語言 的 認識 達 到 母語者 程度 的 機會 是 相當 小 的   。   <end>\n",
            "\n",
            "Number of English Sentences:  26388\n",
            "Number of Chinese sentences:  26388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4vgafsXpFFF"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "source": [
        "# create a helper function to get the padded tensor length\n",
        "# the default level(0.98) means that 98% of all sentences have fewer than n tokens\n",
        "def get_pad_len(tensor, level=0.98): \n",
        "    n = 0\n",
        "    while True:\n",
        "        count = 0\n",
        "        for i in tensor:\n",
        "            if len(i) < n:\n",
        "                count += 1\n",
        "        if count / len(tensor) >= level:\n",
        "            break\n",
        "        n += 1\n",
        "    return n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ukpjRBbpsy1"
      },
      "source": [
        "def to_tensor(lang, return_tensor=True, return_tokenizer=False):\n",
        "    # Assigns the index (sequence) of each word in a text to X\n",
        "    tokenizer = Tokenizer(filters=' ', oov_token='<OOV>') \n",
        "    tokenizer.fit_on_texts(lang)\n",
        "    lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "    lang_tensor = pad_sequences(lang_tensor,\n",
        "                                maxlen=get_pad_len(lang_tensor), # use the previously created function\n",
        "                                padding='post',\n",
        "                                truncating='post') \n",
        "    if return_tensor:\n",
        "        print('\\nShape of data tensor:', lang_tensor.shape)\n",
        "        return lang_tensor\n",
        "    if return_tokenizer:\n",
        "        return tokenizer"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB3f-h8EpwqP",
        "outputId": "ffe32fe2-b91c-434e-bf3a-adcf6bd0f779"
      },
      "source": [
        "english_tokenizer = to_tensor(english, False, True)\n",
        "english_tensor = to_tensor(english)\n",
        "print('\\nOriginal sentence:')\n",
        "print(english[500])\n",
        "print('\\nTensor of the sentence:')\n",
        "print(english_tensor[500])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of data tensor: (26388, 17)\n",
            "\n",
            "Original sentence:\n",
            "<start> tom laughed .  <end>\n",
            "\n",
            "Tensor of the sentence:\n",
            "[  2  12 944   4   3   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj_d4yQ3qfX3",
        "outputId": "0cc22a05-c769-42fe-8307-4f2a2a8b0e3b"
      },
      "source": [
        "\n",
        "chinese_tokenizer = to_tensor(chinese, False, True)\n",
        "chinese_tensor = to_tensor(chinese)\n",
        "print('\\nOriginal sentence:')\n",
        "print(chinese[500])\n",
        "print('\\nTensor of the sentence:')\n",
        "print(chinese_tensor[500])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of data tensor: (26388, 16)\n",
            "\n",
            "Original sentence:\n",
            "<start> 汤姆 笑 了   。   <end>\n",
            "\n",
            "Tensor of the sentence:\n",
            "[  2  14 324   7   4   3   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMQmnjie4mFH",
        "outputId": "d5ae9b3a-ffdf-4249-86e4-710bf481549a"
      },
      "source": [
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "chinese_vocab_size = len(chinese_tokenizer.word_index) + 1\n",
        "\n",
        "print('Found {} unique tokens in English.\\n'.format(english_vocab_size))\n",
        "print('Found {} unique tokens in Chinese.\\n'.format(chinese_vocab_size))\n",
        "\n",
        "name_dict = ['Chinese', 'English']\n",
        "for idx, lang in enumerate([chinese_tokenizer, english_tokenizer]):\n",
        "    print('The 10 most frequent tokens in {} are:'.format(name_dict[idx]))\n",
        "    for idx, word in enumerate(lang.word_index):\n",
        "        if word not in ['<OOV>' , '<start>', '<end>']:\n",
        "            print(word, end='|')\n",
        "        if idx == 13:\n",
        "            break\n",
        "    print('\\n')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6756 unique tokens in English.\n",
            "\n",
            "Found 15400 unique tokens in Chinese.\n",
            "\n",
            "The 10 most frequent tokens in Chinese are:\n",
            "。|我|的|了|你|他|？|在|是|她|汤姆|\n",
            "\n",
            "The 10 most frequent tokens in English are:\n",
            ".|i|the|to|you|a|?|is|tom|t|he|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbUR8g9R4wDh"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbY0N5Ht4vJ5"
      },
      "source": [
        "english_train, english_test, chinese_train, chinese_test = train_test_split(\n",
        "    english_tensor, chinese_tensor, test_size=0.1)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "BUFFER_SIZE = len(english_train)\n",
        "BATCH_SIZE = 64\n",
        "STEPS_PER_EPOCH = len(english_train)//BATCH_SIZE\n",
        "EMBEDDING_DIM = 128\n",
        "ENC_HIDDEN_DIM = 1024\n",
        "DEC_HIDDEN_DIM = 1024\n",
        "\n",
        "# creating a TensorFlow Dataset object \n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_train, chinese_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# batching\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe87b228-7f0f-4d13-ea51-3c0950c60eaa"
      },
      "source": [
        "print(chinese_vocab_size)\n",
        "print(english_vocab_size)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15400\n",
            "6756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYs4fsBh5Ucv"
      },
      "source": [
        "# Encoder-decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_hidden_dim, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_hidden_dim = enc_hidden_dim\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    # 1. Добавил  tf.keras.layers.Dropout в encoder для нормализации.\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_hidden_dim,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "  def call(self, x, init_state, training=True):\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    output, hidden = self.gru(x, initial_state = init_state)\n",
        "    return output, hidden\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_hidden_dim))"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "  def call(self, x, hidden, drop=True):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQaITYQ455vW"
      },
      "source": [
        "# Instantiate the models with sample batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "source": [
        "\n",
        "encoder = Encoder(english_vocab_size, EMBEDDING_DIM, ENC_HIDDEN_DIM, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKcypC0AGeLR",
        "outputId": "250adb08-2a85-484f-93b9-2ba4c7b28917"
      },
      "source": [
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 17, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y0HF-zMF_vp",
        "outputId": "3650c3e6-e022-48b5-9525-98c6d154cd97"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     multiple                  864768    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "gru_16 (GRU)                 multiple                  3545088   \n",
            "=================================================================\n",
            "Total params: 4,409,856\n",
            "Trainable params: 4,409,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oj1KwaQ8Rp2",
        "outputId": "4892eca1-14e2-4bc6-c0f1-45bb772ec297"
      },
      "source": [
        "decoder = Decoder(chinese_vocab_size, EMBEDDING_DIM, DEC_HIDDEN_DIM)\n",
        "\n",
        "sample_decoder_output, _,  = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 15400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS_eIbeG_E06",
        "outputId": "26ec0648-6e0f-4cbf-93d8-16087b57fa2f"
      },
      "source": [
        "decoder.summary()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     multiple                  1971200   \n",
            "_________________________________________________________________\n",
            "gru_17 (GRU)                 multiple                  3545088   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              multiple                  15785000  \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         multiple                  0         \n",
            "=================================================================\n",
            "Total params: 21,301,288\n",
            "Trainable params: 21,301,288\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_nmt_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMjsyyWX_T6L"
      },
      "source": [
        "Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddefjBMa3jF0",
        "outputId": "8db8fbd0-841b-41e0-f75c-ede1ec157ea8"
      },
      "source": [
        "with tf.device(\"/gpu:0\"):\n",
        "    EPOCHS = 50\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "            batch_loss = train_step(inp, targ, enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                        batch,\n",
        "                                                        batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.4538\n",
            "Epoch 1 Batch 100 Loss 2.4744\n",
            "Epoch 1 Batch 200 Loss 2.4372\n",
            "Epoch 1 Batch 300 Loss 2.3960\n",
            "Epoch 2 Batch 0 Loss 2.3406\n",
            "Epoch 2 Batch 100 Loss 2.2141\n",
            "Epoch 2 Batch 200 Loss 2.0997\n",
            "Epoch 2 Batch 300 Loss 1.8546\n",
            "Epoch 3 Batch 0 Loss 1.7161\n",
            "Epoch 3 Batch 100 Loss 1.7174\n",
            "Epoch 3 Batch 200 Loss 1.7089\n",
            "Epoch 3 Batch 300 Loss 1.6948\n",
            "Epoch 4 Batch 0 Loss 1.5063\n",
            "Epoch 4 Batch 100 Loss 1.5544\n",
            "Epoch 4 Batch 200 Loss 1.5660\n",
            "Epoch 4 Batch 300 Loss 1.4226\n",
            "Epoch 5 Batch 0 Loss 1.1708\n",
            "Epoch 5 Batch 100 Loss 1.2091\n",
            "Epoch 5 Batch 200 Loss 1.1144\n",
            "Epoch 5 Batch 300 Loss 1.1131\n",
            "Epoch 6 Batch 0 Loss 0.9555\n",
            "Epoch 6 Batch 100 Loss 0.8802\n",
            "Epoch 6 Batch 200 Loss 0.8918\n",
            "Epoch 6 Batch 300 Loss 0.9121\n",
            "Epoch 7 Batch 0 Loss 0.5963\n",
            "Epoch 7 Batch 100 Loss 0.6181\n",
            "Epoch 7 Batch 200 Loss 0.7068\n",
            "Epoch 7 Batch 300 Loss 0.6509\n",
            "Epoch 8 Batch 0 Loss 0.4448\n",
            "Epoch 8 Batch 100 Loss 0.5201\n",
            "Epoch 8 Batch 200 Loss 0.5165\n",
            "Epoch 8 Batch 300 Loss 0.4915\n",
            "Epoch 9 Batch 0 Loss 0.3804\n",
            "Epoch 9 Batch 100 Loss 0.4226\n",
            "Epoch 9 Batch 200 Loss 0.3989\n",
            "Epoch 9 Batch 300 Loss 0.3994\n",
            "Epoch 10 Batch 0 Loss 0.2492\n",
            "Epoch 10 Batch 100 Loss 0.2536\n",
            "Epoch 10 Batch 200 Loss 0.3222\n",
            "Epoch 10 Batch 300 Loss 0.3186\n",
            "Epoch 11 Batch 0 Loss 0.1526\n",
            "Epoch 11 Batch 100 Loss 0.2137\n",
            "Epoch 11 Batch 200 Loss 0.1978\n",
            "Epoch 11 Batch 300 Loss 0.2323\n",
            "Epoch 12 Batch 0 Loss 0.1291\n",
            "Epoch 12 Batch 100 Loss 0.1758\n",
            "Epoch 12 Batch 200 Loss 0.1720\n",
            "Epoch 12 Batch 300 Loss 0.1850\n",
            "Epoch 13 Batch 0 Loss 0.0946\n",
            "Epoch 13 Batch 100 Loss 0.1075\n",
            "Epoch 13 Batch 200 Loss 0.1018\n",
            "Epoch 13 Batch 300 Loss 0.1180\n",
            "Epoch 14 Batch 0 Loss 0.0621\n",
            "Epoch 14 Batch 100 Loss 0.0811\n",
            "Epoch 14 Batch 200 Loss 0.0833\n",
            "Epoch 14 Batch 300 Loss 0.1132\n",
            "Epoch 15 Batch 0 Loss 0.0558\n",
            "Epoch 15 Batch 100 Loss 0.0565\n",
            "Epoch 15 Batch 200 Loss 0.0728\n",
            "Epoch 15 Batch 300 Loss 0.0872\n",
            "Epoch 16 Batch 0 Loss 0.0485\n",
            "Epoch 16 Batch 100 Loss 0.0638\n",
            "Epoch 16 Batch 200 Loss 0.0561\n",
            "Epoch 16 Batch 300 Loss 0.0565\n",
            "Epoch 17 Batch 0 Loss 0.0466\n",
            "Epoch 17 Batch 100 Loss 0.0510\n",
            "Epoch 17 Batch 200 Loss 0.0486\n",
            "Epoch 17 Batch 300 Loss 0.0838\n",
            "Epoch 18 Batch 0 Loss 0.0448\n",
            "Epoch 18 Batch 100 Loss 0.0321\n",
            "Epoch 18 Batch 200 Loss 0.0236\n",
            "Epoch 18 Batch 300 Loss 0.0510\n",
            "Epoch 19 Batch 0 Loss 0.0355\n",
            "Epoch 19 Batch 100 Loss 0.0355\n",
            "Epoch 19 Batch 200 Loss 0.0446\n",
            "Epoch 19 Batch 300 Loss 0.0573\n",
            "Epoch 20 Batch 0 Loss 0.0358\n",
            "Epoch 20 Batch 100 Loss 0.0200\n",
            "Epoch 20 Batch 200 Loss 0.0506\n",
            "Epoch 20 Batch 300 Loss 0.0742\n",
            "Epoch 21 Batch 0 Loss 0.0474\n",
            "Epoch 21 Batch 100 Loss 0.0417\n",
            "Epoch 21 Batch 200 Loss 0.0781\n",
            "Epoch 21 Batch 300 Loss 0.0935\n",
            "Epoch 22 Batch 0 Loss 0.0720\n",
            "Epoch 22 Batch 100 Loss 0.0854\n",
            "Epoch 22 Batch 200 Loss 0.0752\n",
            "Epoch 22 Batch 300 Loss 0.0844\n",
            "Epoch 23 Batch 0 Loss 0.0378\n",
            "Epoch 23 Batch 100 Loss 0.0540\n",
            "Epoch 23 Batch 200 Loss 0.0732\n",
            "Epoch 23 Batch 300 Loss 0.0496\n",
            "Epoch 24 Batch 0 Loss 0.0244\n",
            "Epoch 24 Batch 100 Loss 0.0311\n",
            "Epoch 24 Batch 200 Loss 0.0212\n",
            "Epoch 24 Batch 300 Loss 0.0229\n",
            "Epoch 25 Batch 0 Loss 0.0581\n",
            "Epoch 25 Batch 100 Loss 0.0361\n",
            "Epoch 25 Batch 200 Loss 0.0427\n",
            "Epoch 25 Batch 300 Loss 0.0406\n",
            "Epoch 26 Batch 0 Loss 0.0259\n",
            "Epoch 26 Batch 100 Loss 0.0312\n",
            "Epoch 26 Batch 200 Loss 0.0170\n",
            "Epoch 26 Batch 300 Loss 0.0606\n",
            "Epoch 27 Batch 0 Loss 0.0188\n",
            "Epoch 27 Batch 100 Loss 0.0111\n",
            "Epoch 27 Batch 200 Loss 0.0424\n",
            "Epoch 27 Batch 300 Loss 0.0200\n",
            "Epoch 28 Batch 0 Loss 0.0157\n",
            "Epoch 28 Batch 100 Loss 0.0425\n",
            "Epoch 28 Batch 200 Loss 0.0268\n",
            "Epoch 28 Batch 300 Loss 0.0383\n",
            "Epoch 29 Batch 0 Loss 0.0166\n",
            "Epoch 29 Batch 100 Loss 0.0571\n",
            "Epoch 29 Batch 200 Loss 0.0142\n",
            "Epoch 29 Batch 300 Loss 0.0320\n",
            "Epoch 30 Batch 0 Loss 0.0421\n",
            "Epoch 30 Batch 100 Loss 0.0178\n",
            "Epoch 30 Batch 200 Loss 0.0213\n",
            "Epoch 30 Batch 300 Loss 0.0411\n",
            "Epoch 31 Batch 0 Loss 0.0359\n",
            "Epoch 31 Batch 100 Loss 0.0205\n",
            "Epoch 31 Batch 200 Loss 0.0442\n",
            "Epoch 31 Batch 300 Loss 0.0668\n",
            "Epoch 32 Batch 0 Loss 0.0668\n",
            "Epoch 32 Batch 100 Loss 0.0555\n",
            "Epoch 32 Batch 200 Loss 0.0667\n",
            "Epoch 32 Batch 300 Loss 0.0774\n",
            "Epoch 33 Batch 0 Loss 0.0558\n",
            "Epoch 33 Batch 100 Loss 0.1040\n",
            "Epoch 33 Batch 200 Loss 0.0622\n",
            "Epoch 33 Batch 300 Loss 0.1126\n",
            "Epoch 34 Batch 0 Loss 0.0272\n",
            "Epoch 34 Batch 100 Loss 0.0324\n",
            "Epoch 34 Batch 200 Loss 0.0400\n",
            "Epoch 34 Batch 300 Loss 0.0700\n",
            "Epoch 35 Batch 0 Loss 0.0274\n",
            "Epoch 35 Batch 100 Loss 0.0460\n",
            "Epoch 35 Batch 200 Loss 0.0435\n",
            "Epoch 35 Batch 300 Loss 0.0320\n",
            "Epoch 36 Batch 0 Loss 0.0151\n",
            "Epoch 36 Batch 100 Loss 0.0244\n",
            "Epoch 36 Batch 200 Loss 0.0386\n",
            "Epoch 36 Batch 300 Loss 0.0238\n",
            "Epoch 37 Batch 0 Loss 0.0343\n",
            "Epoch 37 Batch 100 Loss 0.0202\n",
            "Epoch 37 Batch 200 Loss 0.0232\n",
            "Epoch 37 Batch 300 Loss 0.0327\n",
            "Epoch 38 Batch 0 Loss 0.0096\n",
            "Epoch 38 Batch 100 Loss 0.0380\n",
            "Epoch 38 Batch 200 Loss 0.0236\n",
            "Epoch 38 Batch 300 Loss 0.0266\n",
            "Epoch 39 Batch 0 Loss 0.0239\n",
            "Epoch 39 Batch 100 Loss 0.0292\n",
            "Epoch 39 Batch 200 Loss 0.0258\n",
            "Epoch 39 Batch 300 Loss 0.0328\n",
            "Epoch 40 Batch 0 Loss 0.0052\n",
            "Epoch 40 Batch 100 Loss 0.0230\n",
            "Epoch 40 Batch 200 Loss 0.0472\n",
            "Epoch 40 Batch 300 Loss 0.0293\n",
            "Epoch 41 Batch 0 Loss 0.0090\n",
            "Epoch 41 Batch 100 Loss 0.0383\n",
            "Epoch 41 Batch 200 Loss 0.0245\n",
            "Epoch 41 Batch 300 Loss 0.0618\n",
            "Epoch 42 Batch 0 Loss 0.0229\n",
            "Epoch 42 Batch 100 Loss 0.0266\n",
            "Epoch 42 Batch 200 Loss 0.0323\n",
            "Epoch 42 Batch 300 Loss 0.0676\n",
            "Epoch 43 Batch 0 Loss 0.0340\n",
            "Epoch 43 Batch 100 Loss 0.0540\n",
            "Epoch 43 Batch 200 Loss 0.0864\n",
            "Epoch 43 Batch 300 Loss 0.0929\n",
            "Epoch 44 Batch 0 Loss 0.0577\n",
            "Epoch 44 Batch 100 Loss 0.0657\n",
            "Epoch 44 Batch 200 Loss 0.1186\n",
            "Epoch 44 Batch 300 Loss 0.1080\n",
            "Epoch 45 Batch 0 Loss 0.0465\n",
            "Epoch 45 Batch 100 Loss 0.0467\n",
            "Epoch 45 Batch 200 Loss 0.0692\n",
            "Epoch 45 Batch 300 Loss 0.0865\n",
            "Epoch 46 Batch 0 Loss 0.0459\n",
            "Epoch 46 Batch 100 Loss 0.0421\n",
            "Epoch 46 Batch 200 Loss 0.0673\n",
            "Epoch 46 Batch 300 Loss 0.0423\n",
            "Epoch 47 Batch 0 Loss 0.0269\n",
            "Epoch 47 Batch 100 Loss 0.0336\n",
            "Epoch 47 Batch 200 Loss 0.0292\n",
            "Epoch 47 Batch 300 Loss 0.0218\n",
            "Epoch 48 Batch 0 Loss 0.0198\n",
            "Epoch 48 Batch 100 Loss 0.0250\n",
            "Epoch 48 Batch 200 Loss 0.0198\n",
            "Epoch 48 Batch 300 Loss 0.0163\n",
            "Epoch 49 Batch 0 Loss 0.0197\n",
            "Epoch 49 Batch 100 Loss 0.0155\n",
            "Epoch 49 Batch 200 Loss 0.0280\n",
            "Epoch 49 Batch 300 Loss 0.0291\n",
            "Epoch 50 Batch 0 Loss 0.0263\n",
            "Epoch 50 Batch 100 Loss 0.0252\n",
            "Epoch 50 Batch 200 Loss 0.0291\n",
            "Epoch 50 Batch 300 Loss 0.0356\n",
            "Epoch 50 Loss 0.0256\n",
            "Time taken for 1 epoch 18.151574850082397 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjSNKSPWJb6a",
        "outputId": "472659c5-3265-4239-dc5d-eda470f75df5"
      },
      "source": [
        "inp_lang"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f8b26ab5b10>"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaibv3x1kE1f"
      },
      "source": [
        "def get_eval_tensor(sentence):\n",
        "    processed_sentence = preprocess_sentence(sentence)\n",
        "    try:\n",
        "        inputs = [english_tokenizer.word_index[i] for i in processed_sentence.split(' ')]\n",
        "        inputs = pad_sequences([inputs], maxlen=english_tensor.shape[1], padding='post')\n",
        "        inputs = tf.convert_to_tensor(inputs)\n",
        "        return inputs\n",
        "    except:\n",
        "        return [0 for _ in processed_sentence.split(' ')]\n",
        "        # print('The Neural Network has not learned the word yet!')\n",
        "    \n",
        "def evaluate(sentence):\n",
        "    result = ''\n",
        "    # initialize encoder hidden layer\n",
        "    hidden = [tf.zeros((1, DEC_HIDDEN_DIM))]\n",
        "    enc_out, enc_hidden = encoder(get_eval_tensor(sentence), hidden, False)\n",
        "    # False means not using dropout\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([chinese_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    attention_plot = np.zeros((chinese_tensor.shape[1], english_tensor.shape[1]))\n",
        "    \n",
        "\n",
        "    for t in range(english_tensor.shape[1]):\n",
        "        predictions, dec_hidden = decoder(\n",
        "            dec_input, dec_hidden, enc_out) \n",
        "        # False means not using dropout \n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "\n",
        "        try:\n",
        "            new_word = chinese_tokenizer.index_word[predicted_id] \n",
        "        except:\n",
        "            pass\n",
        "       \n",
        "        result += new_word + ' '\n",
        "        if chinese_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, attention_plot"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHBgmx07jz9A"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = go.Figure(data = go.Heatmap(z=attention[:-1,:],\n",
        "                                      x=sentence,\n",
        "                                      y=[i for i in predicted_sentence[:-1][::-1]]))\n",
        "    fig.update_xaxes(side=\"top\")\n",
        "    fig.update_layout(\n",
        "        autosize=False,\n",
        "        width=700,\n",
        "        height=500,\n",
        "        margin=dict(l=50, r=50, b=5, t=2, pad=4)\n",
        "        )\n",
        "    fig.show()"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32XnbFUKa8o5"
      },
      "source": [
        "def translate(sentence, plot=False, score=False):\n",
        "    result, attention_plot = evaluate(sentence)\n",
        "    if score == False:\n",
        "        print(result.replace(' ', '').strip('<end>'))\n",
        "    if score:\n",
        "        return result.strip('<end> ')\n",
        "    if plot:\n",
        "        attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "        plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJpT9D5_OgP6",
        "outputId": "24cb5b6a-a95b-4167-c6af-76216f7c7814"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8a590cb6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bhFfwcIMX5i"
      },
      "source": [
        "ENC_HIDDEN_DIM = 1024\n",
        "DEC_HIDDEN_DIM = 1024"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOrEqYzhNlba",
        "outputId": "d3dad5c6-bfa2-4bae-d1c7-aae7eb53bcaf"
      },
      "source": [
        "translate('He is the only person I know')"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "他是我唯一的房子。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF-xeGRmcJFz"
      },
      "source": [
        "# Перевел как он моя единственная квартира ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM5BrQlbN0rh",
        "outputId": "eab35fbe-cb3d-49a6-d632-a55e5ad394af"
      },
      "source": [
        "translate('I am very hungry')\n",
        "# зачет"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很餓。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "9LF_XZ6jT16X",
        "outputId": "2587791a-7536-45dd-90a2-6642210385a8"
      },
      "source": [
        "translate(u'The default initial hidden')"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-292-fccb3bbea6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'The default initial hidden'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-283-0c0d590b40ce>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, plot, score)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-281-beb037786055>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# initialize encoder hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_HIDDEN_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_eval_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# False means not using dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-fb51c0db914b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, training)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int32'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m   \"\"\"\n\u001b[0;32m-> 1483\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09_hUFx9EJh",
        "outputId": "1f4c66a7-81c6-4507-ecb4-370c5a35027c"
      },
      "source": [
        "translate(u'It goes without saying')\n",
        "# don't correct"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "不言而喻。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7c5p8rmkHQG",
        "outputId": "c4279a1e-7afc-4a63-e3dc-f865bf688dec"
      },
      "source": [
        "translate(u'We must keep books clean ')\n",
        "# don't correct"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我们必须走了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE6ywT_fQLxu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}